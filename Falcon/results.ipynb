{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge \n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read excel file\n",
    "df = pd.read_excel('../results/falcon-7b-NOT-FOUND-REMOVED-firstpart-4epoch-lr4e-4.xlsx')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88977fc0eb78179a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa3024751b48b02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f_scores=[]\n",
    "p_scores=[]\n",
    "r_scores=[]\n",
    "structures=[]\n",
    "cosine_scores=[]\n",
    "is_includeds = []\n",
    "report_types = []\n",
    "for  row in df.itertuples():\n",
    "   \n",
    "    rouge_L_f=[]\n",
    "    rouge_1_p=[]\n",
    "    rouge_1_r=[]\n",
    "    is_included = []\n",
    "    cosine_score=[]\n",
    "    possible_asnwers = row.answer.split('|')\n",
    "    possible_asnwers = [answer.strip() for answer in possible_asnwers]\n",
    "    \n",
    "    generated_predictions = row.prediction.split('|')\n",
    "    generated_predictions = [prediction.strip() for prediction in generated_predictions]\n",
    "    \n",
    "    p=generated_predictions[0]\n",
    "    a=possible_asnwers[0]\n",
    "\n",
    "    if p==\"\" or a==\"\" :\n",
    "        continue\n",
    "    scores = rouge.get_scores(p, a)\n",
    "    rouge_1_p.append(scores[0]['rouge-l']['p'])\n",
    "    rouge_1_r.append(scores[0]['rouge-l']['r'])\n",
    "    rouge_L_f.append(scores[0]['rouge-l']['f'])\n",
    "    is_included.append(a in (p) or p in (a))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    p_scores.append(np.max(rouge_1_p))\n",
    "    r_scores.append(np.max(rouge_1_r))\n",
    "    f_scores.append(np.max(rouge_L_f))\n",
    "    is_includeds.append(np.max(is_included))\n",
    "    structures.append(row.structure)\n",
    "    report_types.append(row.report_type)\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "result_scores= pd.DataFrame({'f_score':f_scores,'p_score':p_scores,'r_score':r_scores,'structure':structures ,'r_type':report_types,'is_included':is_includeds})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88a90a8d131bf5c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "beead7c3dab23ef9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for report_type in result_scores.r_type.unique():\n",
    "    print(\"-----\",report_type,'-------')\n",
    "    for structures in result_scores.structure.unique():\n",
    "        print(\"-----\",structures,'-------')\n",
    "        sub_data = result_scores[(result_scores.structure==structures) & (result_scores.r_type==report_type)]\n",
    "        print(\"f_score_mean\",sub_data.f_score.mean().round(4))\n",
    "        # print(\"f_score_std\",sub_data.f_score.std().round(4))\n",
    "        print(\"p_score_mean\",sub_data.p_score.mean().round(4))\n",
    "        # print(\"p_score_std\",sub_data.p_score.std().round(4))\n",
    "        print(\"r_score_mean\",sub_data.r_score.mean().round(4))\n",
    "        # print(\"r_score_std\",sub_data.r_score.std().round(4))\n",
    "        print(\"is_included_mean\",sub_data.is_included.sum() ,\"out of\",sub_data.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c6c0cba81ca05da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/results_with_tags.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "509ceabdd7e5fbca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "for report_type in df.report_type.unique():\n",
    "    print(\"-----\",report_type,'-------')\n",
    "    for structures in df.structure.unique():\n",
    "        print(\"-----\",structures,'-------')\n",
    "         # calculate multiclass accuracy, precision, recall\n",
    "        sub_data = df[(df.structure==structures) & (df.report_type==report_type)]\n",
    "        y= sub_data[\"structure_score(GT)\"].fillna(1.0).astype(str)\n",
    "        y_pred = sub_data[\"predicted_score\"].fillna(1.0).astype(str)\n",
    "        print(\"accuracy\",accuracy_score(y, y_pred).round(2))\n",
    "        print(\"precision\",precision_score(y, y_pred,average='macro').round(2))\n",
    "        print(\"recall\",recall_score(y, y_pred,average='macro').round(2))\n",
    "        print(\"f1\",f1_score(y, y_pred,average='macro').round(2))\n",
    "        \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b0b93891cf0b84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9e1cdc7eccbb86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
